{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea4a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d85994",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "Vamos a implementar las distintas decoding strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dac12e",
   "metadata": {},
   "source": [
    "# Greedy Search Decoding\n",
    "Cargamos la versión de GPT-2 con 1.5-billion-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693caf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device=\"cpu\"\n",
    "model_name = \"gpt2-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460da52f",
   "metadata": {},
   "source": [
    "**Hugging face Transformers provee una función** `generate()` **para modelos autorregresivos como GPT-2**, pero en este notebook lo implementaremos a mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e86589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most (8.53%)</td>\n",
       "      <td>only (4.96%)</td>\n",
       "      <td>best (4.65%)</td>\n",
       "      <td>Transformers (4.37%)</td>\n",
       "      <td>ultimate (2.16%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the most</td>\n",
       "      <td>popular (16.78%)</td>\n",
       "      <td>powerful (5.37%)</td>\n",
       "      <td>common (4.96%)</td>\n",
       "      <td>famous (3.72%)</td>\n",
       "      <td>successful (3.20%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the most popular</td>\n",
       "      <td>toy (10.63%)</td>\n",
       "      <td>toys (7.23%)</td>\n",
       "      <td>Transformers (6.60%)</td>\n",
       "      <td>of (5.46%)</td>\n",
       "      <td>and (3.76%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the most popular toy</td>\n",
       "      <td>line (34.38%)</td>\n",
       "      <td>in (18.20%)</td>\n",
       "      <td>of (11.71%)</td>\n",
       "      <td>brand (6.10%)</td>\n",
       "      <td>line (2.69%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the most popular toy line</td>\n",
       "      <td>in (46.29%)</td>\n",
       "      <td>of (15.09%)</td>\n",
       "      <td>, (4.94%)</td>\n",
       "      <td>on (4.40%)</td>\n",
       "      <td>ever (2.72%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the most popular toy line in</td>\n",
       "      <td>the (65.99%)</td>\n",
       "      <td>history (12.42%)</td>\n",
       "      <td>America (6.91%)</td>\n",
       "      <td>Japan (2.44%)</td>\n",
       "      <td>North (1.40%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the most popular toy line in the</td>\n",
       "      <td>world (69.27%)</td>\n",
       "      <td>United (4.55%)</td>\n",
       "      <td>history (4.29%)</td>\n",
       "      <td>US (4.23%)</td>\n",
       "      <td>U (2.30%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>, (39.73%)</td>\n",
       "      <td>. (30.64%)</td>\n",
       "      <td>and (9.87%)</td>\n",
       "      <td>with (2.32%)</td>\n",
       "      <td>today (1.74%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>and (32.51%)</td>\n",
       "      <td>with (18.66%)</td>\n",
       "      <td>but (8.06%)</td>\n",
       "      <td>so (5.42%)</td>\n",
       "      <td>having (2.19%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>the (11.70%)</td>\n",
       "      <td>they (7.48%)</td>\n",
       "      <td>it (6.24%)</td>\n",
       "      <td>for (3.06%)</td>\n",
       "      <td>I (2.97%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>Transformers (6.38%)</td>\n",
       "      <td>most (4.67%)</td>\n",
       "      <td>first (3.39%)</td>\n",
       "      <td>franchise (3.01%)</td>\n",
       "      <td>only (2.06%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>are (16.26%)</td>\n",
       "      <td>have (7.27%)</td>\n",
       "      <td>franchise (5.74%)</td>\n",
       "      <td>Universe (4.55%)</td>\n",
       "      <td>: (4.42%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>the (30.39%)</td>\n",
       "      <td>a (3.99%)</td>\n",
       "      <td>one (3.72%)</td>\n",
       "      <td>among (2.07%)</td>\n",
       "      <td>arguably (1.80%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>most (45.68%)</td>\n",
       "      <td>best (5.27%)</td>\n",
       "      <td>biggest (2.35%)</td>\n",
       "      <td>stars (2.15%)</td>\n",
       "      <td>coolest (1.27%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>popular (62.53%)</td>\n",
       "      <td>famous (4.76%)</td>\n",
       "      <td>recognizable (3.89%)</td>\n",
       "      <td>successful (3.71%)</td>\n",
       "      <td>iconic (2.07%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>toy (23.44%)</td>\n",
       "      <td>toys (19.79%)</td>\n",
       "      <td>Transformers (4.64%)</td>\n",
       "      <td>characters (4.02%)</td>\n",
       "      <td>movie (2.92%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>line (35.05%)</td>\n",
       "      <td>in (21.34%)</td>\n",
       "      <td>brand (3.85%)</td>\n",
       "      <td>characters (3.84%)</td>\n",
       "      <td>of (3.73%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>in (75.57%)</td>\n",
       "      <td>of (6.91%)</td>\n",
       "      <td>ever (2.10%)</td>\n",
       "      <td>. (2.05%)</td>\n",
       "      <td>on (1.59%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>the (78.00%)</td>\n",
       "      <td>America (3.79%)</td>\n",
       "      <td>history (2.80%)</td>\n",
       "      <td>Japan (2.24%)</td>\n",
       "      <td>all (1.59%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>world (77.78%)</td>\n",
       "      <td>universe (5.24%)</td>\n",
       "      <td>history (2.01%)</td>\n",
       "      <td>United (1.62%)</td>\n",
       "      <td>entire (1.20%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Input               Choice 1  \\\n",
       "0                                Transformers are the           most (8.53%)   \n",
       "1                           Transformers are the most       popular (16.78%)   \n",
       "2                   Transformers are the most popular           toy (10.63%)   \n",
       "3               Transformers are the most popular toy          line (34.38%)   \n",
       "4          Transformers are the most popular toy line            in (46.29%)   \n",
       "5       Transformers are the most popular toy line in           the (65.99%)   \n",
       "6   Transformers are the most popular toy line in the         world (69.27%)   \n",
       "7   Transformers are the most popular toy line in ...             , (39.73%)   \n",
       "8   Transformers are the most popular toy line in ...           and (32.51%)   \n",
       "9   Transformers are the most popular toy line in ...           the (11.70%)   \n",
       "10  Transformers are the most popular toy line in ...   Transformers (6.38%)   \n",
       "11  Transformers are the most popular toy line in ...           are (16.26%)   \n",
       "12  Transformers are the most popular toy line in ...           the (30.39%)   \n",
       "13  Transformers are the most popular toy line in ...          most (45.68%)   \n",
       "14  Transformers are the most popular toy line in ...       popular (62.53%)   \n",
       "15  Transformers are the most popular toy line in ...           toy (23.44%)   \n",
       "16  Transformers are the most popular toy line in ...          line (35.05%)   \n",
       "17  Transformers are the most popular toy line in ...            in (75.57%)   \n",
       "18  Transformers are the most popular toy line in ...           the (78.00%)   \n",
       "19  Transformers are the most popular toy line in ...         world (77.78%)   \n",
       "\n",
       "             Choice 2               Choice 3               Choice 4  \\\n",
       "0        only (4.96%)           best (4.65%)   Transformers (4.37%)   \n",
       "1    powerful (5.37%)         common (4.96%)         famous (3.72%)   \n",
       "2        toys (7.23%)   Transformers (6.60%)             of (5.46%)   \n",
       "3         in (18.20%)            of (11.71%)          brand (6.10%)   \n",
       "4         of (15.09%)              , (4.94%)             on (4.40%)   \n",
       "5    history (12.42%)        America (6.91%)          Japan (2.44%)   \n",
       "6      United (4.55%)        history (4.29%)             US (4.23%)   \n",
       "7          . (30.64%)            and (9.87%)           with (2.32%)   \n",
       "8       with (18.66%)            but (8.06%)             so (5.42%)   \n",
       "9        they (7.48%)             it (6.24%)            for (3.06%)   \n",
       "10       most (4.67%)          first (3.39%)      franchise (3.01%)   \n",
       "11       have (7.27%)      franchise (5.74%)       Universe (4.55%)   \n",
       "12          a (3.99%)            one (3.72%)          among (2.07%)   \n",
       "13       best (5.27%)        biggest (2.35%)          stars (2.15%)   \n",
       "14     famous (4.76%)   recognizable (3.89%)     successful (3.71%)   \n",
       "15      toys (19.79%)   Transformers (4.64%)     characters (4.02%)   \n",
       "16        in (21.34%)          brand (3.85%)     characters (3.84%)   \n",
       "17         of (6.91%)           ever (2.10%)              . (2.05%)   \n",
       "18    America (3.79%)        history (2.80%)          Japan (2.24%)   \n",
       "19   universe (5.24%)        history (2.01%)         United (1.62%)   \n",
       "\n",
       "               Choice 5  \n",
       "0      ultimate (2.16%)  \n",
       "1    successful (3.20%)  \n",
       "2           and (3.76%)  \n",
       "3          line (2.69%)  \n",
       "4          ever (2.72%)  \n",
       "5         North (1.40%)  \n",
       "6             U (2.30%)  \n",
       "7         today (1.74%)  \n",
       "8        having (2.19%)  \n",
       "9             I (2.97%)  \n",
       "10         only (2.06%)  \n",
       "11            : (4.42%)  \n",
       "12     arguably (1.80%)  \n",
       "13      coolest (1.27%)  \n",
       "14       iconic (2.07%)  \n",
       "15        movie (2.92%)  \n",
       "16           of (3.73%)  \n",
       "17           on (1.59%)  \n",
       "18          all (1.59%)  \n",
       "19       entire (1.20%)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_txt = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 20\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        # Select logits of the first batch and the last token and apply softmax\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        \n",
    "        # Store tokens with highest probabilities\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        \n",
    "        # Append predicted next token to input\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "\n",
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b307dc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transformers are the',\n",
       " 'Transformers are the most',\n",
       " 'Transformers are the most popular',\n",
       " 'Transformers are the most popular toy',\n",
       " 'Transformers are the most popular toy line',\n",
       " 'Transformers are the most popular toy line in',\n",
       " 'Transformers are the most popular toy line in the',\n",
       " 'Transformers are the most popular toy line in the world',\n",
       " 'Transformers are the most popular toy line in the world,',\n",
       " 'Transformers are the most popular toy line in the world, and',\n",
       " 'Transformers are the most popular toy line in the world, and the',\n",
       " 'Transformers are the most popular toy line in the world, and the Transformers',\n",
       " 'Transformers are the most popular toy line in the world, and the Transformers are',\n",
       " 'Transformers are the most popular toy line in the world, and the Transformers are the',\n",
       " 'Transformers are the most popular toy line in the world, and the Transformers are the most',\n",
       " 'Transformers are the most popular toy line in the world, and the Transformers are the most popular',\n",
       " 'Transformers are the most popular toy line in the world, and the Transformers are the most popular toy',\n",
       " 'Transformers are the most popular toy line in the world, and the Transformers are the most popular toy line',\n",
       " 'Transformers are the most popular toy line in the world, and the Transformers are the most popular toy line in',\n",
       " 'Transformers are the most popular toy line in the world, and the Transformers are the most popular toy line in the']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pd.DataFrame(iterations)[\"Input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f87f102",
   "metadata": {},
   "source": [
    "Vamos a utilizar la función `generate()` para generar secuencias de manera más sofisticada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bcfd528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the most popular toy line in the world, and the Transformers are the most popular toy line in the world\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c9e13",
   "metadata": {},
   "source": [
    "Probamos con un ejemplo algo más elaborado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c85802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n",
      "\n",
      "\n",
      "The researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans.\n",
      "\n",
      "\n",
      "The researchers were surprised to find that the unicorns were able\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738de099",
   "metadata": {},
   "source": [
    "Puede observarse que, a partir de cierto punto, la misma frase empieza a repetirse una y otra vez. Este es uno de los mayores defectos del approach greedy, tiende a producir salidas repetitivas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba8b46",
   "metadata": {},
   "source": [
    "# Beam Search Decoding\n",
    "Comparamos multiplicación con suma de logaritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb8ac05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.562684646268003e-309"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Supongamos que tenemos una secuencia de 1024 tokens, cada uno con una probabilidad de 0.5\n",
    "0.5 ** 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec234ddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-709.7827128933695"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([np.log(0.5)] *1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8120e1",
   "metadata": {},
   "source": [
    "Hugging face Transformers devuelve logits sin normalizar, por lo que implementamos el siguiente código para normalizarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3343ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c56299",
   "metadata": {},
   "source": [
    "Ese código devuelve la log probability de un solo token, el siguiente código devuelve la log probability de la secuencia completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7fcac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :], labels[:, 1:]\n",
    "        )\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b065488",
   "metadata": {},
   "source": [
    "Usamos estas funciones para calcular la log probability de la secuencia obtenida usando el greedy decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1e286c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n",
      "\n",
      "\n",
      "The researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans.\n",
      "\n",
      "\n",
      "The researchers were surprised to find that the unicorns were able\n",
      "\n",
      "log-prob: -87.43\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491e5f1",
   "metadata": {},
   "source": [
    "Comparamos con una secuencia generada usando beam search. Para usar beam_search, simplemente tenemos que indicar el número de beams usando el parámetro `num_beams`. El resultado será generalmente mejor cuantas más beams usemos, pero será computacionalmente más caro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28500a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The discovery of the unicorns was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.\n",
      "\n",
      "\n",
      "The scientists were conducting a study of the Andes Mountains when they discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English\n",
      "\n",
      "log-prob: -55.23\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)\n",
    "\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0267c6",
   "metadata": {},
   "source": [
    "Podemos ver que obtenemos una mejor log probability (un valor más alto es mejor), pero siguen existiendo elementos repetitivos. Podemos configurar el generador de Hugging face para que penalice n-grams repetidos usando el parámetro `no_repeat_ngram_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37e05e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The discovery was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.\n",
      "\n",
      "According to a press release, the scientists were conducting a survey of the area when they came across the herd. They were surprised to find that they were able to converse with the animals in English, even though they had never seen a unicorn in person before. The researchers were\n",
      "\n",
      "log-prob: -93.12\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a860f1ed",
   "metadata": {},
   "source": [
    "El score obtenido es menor, pero el texto se mantiene coherente. Este parámetro es útil para encontrar un buen trade-off entre tokens con altas probabilidades y reducir repeticiones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ecfcea",
   "metadata": {},
   "source": [
    "# Sampling methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1957978b",
   "metadata": {},
   "source": [
    "Ejemplo con $T=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8bb74c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "Heloff released death assumed Indigenous 2003 Belg work Tweet monkey Secondary 457 centuries cmShort partingEv phil KILL Started worker steps Sm stock stalk Showbirds SpikePref suspension symmmopolitan < morphology CPUs Brand storyhow plang Orchestra Wait Professional specialty thirstIm counted hexoma� mappingHost Third latitudeChe biotech longtime Picacles Crit immature influx 141Advertisements want growing bursts 239Leaks To empoweredonna Nit Possible repe;rum Brand 142 downstream\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=2.0, top_k=0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af861a1f",
   "metadata": {},
   "source": [
    "Una temperatura tan alta ha resultado en un texto carente de sentido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060f4cd9",
   "metadata": {},
   "source": [
    "Probamos con $T=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdb0641c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "In fact, the scientists found that the unicorns were able to communicate with each other in English and even taught one another a few words. The scientists also discovered that the unicorns were able to use their voices to communicate with each other.\n",
      "\n",
      "The scientists also discovered that the unicorns are not only able to communicate with each other, but also with humans. This means that the unicorns are able\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=0.5, top_k=0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad816ed",
   "metadata": {},
   "source": [
    "Este texto es bastante más coherente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e64c26",
   "metadata": {},
   "source": [
    "# Top-k y Nucleus Sampling\n",
    "Simplemente usamos el argumento `top_k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfe0df20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "\"We discovered that the unicorns had learned English as infants, which is unusual in the wild. We discovered a herd living in a very isolated location in the mountains, where they could not be contacted by humans,\" said researcher Dr. Roberta Belda-Dobler, from the University of the Andes in Chile.\n",
      "\n",
      "\n",
      "The scientists observed the adult unicorns during their entire breeding cycle,\n"
     ]
    }
   ],
   "source": [
    "output_topk = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=50)\n",
    "\n",
    "print(tokenizer.decode(output_topk[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd4314",
   "metadata": {},
   "source": [
    "Este es posiblemente el texto más humano generado hasta ahora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82242df5",
   "metadata": {},
   "source": [
    "Para utilizar Nucleus, simplemente empleamos el parámetro `top_p`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b1beacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "\"They seemed to have more human qualities and they spoke in an almost perfect English, \"says Juan Carlos Mola, researcher at the University of Oaxaca. \"In my whole life I have never seen anything like this before in human beings.\"\n",
      "\n",
      "\n",
      "According to Mola, it wasn't too long ago that these mythical creatures only existed in legends and myths. \"They have been known as\n"
     ]
    }
   ],
   "source": [
    "output_topp = model.generate(input_ids, max_length=max_length, do_sample=True, top_p=0.9)\n",
    "\n",
    "print(tokenizer.decode(output_topp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4279b9c6",
   "metadata": {},
   "source": [
    "Es posible combinar los parámetros `top_k` y `top_p` en un mismo generador."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
